diff -rbEu llama.cpp.old/llama.cpp llama.cpp/llama.cpp
--- llama.cpp.old/llama.cpp	2024-06-17 21:50:45.399291434 -0400
+++ llama.cpp/llama.cpp	2024-06-17 21:49:25.492292674 -0400
@@ -2161,7 +2161,7 @@
     std::unordered_map<token, id> token_to_id;
     std::vector<token_data>       id_to_token;
 
-    std::vector<id>    cache_special_tokens;
+    std::unordered_map<token, id> cache_special_tokens;
     std::vector<token> cache_token_to_piece; // llama_token_to_piece(special = true);
 
     std::map<std::pair<std::string, std::string>, int> bpe_ranks;
@@ -4845,19 +4845,97 @@
 
     // build special tokens cache
     {
-        for (llama_vocab::id id = 0; id < (llama_vocab::id)n_vocab; ++id) {
+	// TODO: It is unclear (to me) at this point, whether special tokes are guaranteed to be of a deterministic type,
+	//  and will always be correctly labeled in 'added_tokens.json' etc.
+	// The assumption is, since special tokens aren't meant to be exposed to end user, they are designed
+	//  to be unmatchable by the tokenizer, therefore tokens from the vocab, which are unmatchable by the tokenizer
+	//  are special tokens.
+	// From testing, this appears to correlate 1:1 with special tokens.
+	//
+
+	// Counting special tokens and verifying in only one direction
+	//  is sufficient to detect difference in those two sets.
+	//
+	uint32_t special_tokens_count_by_type = 0;
+	uint32_t special_tokens_count_from_verification = 0;
+
+	bool special_tokens_definition_mismatch = false;
+
+	for (const auto & t : vocab.token_to_id) {
+	    const auto & token = t.first;
+	    const auto & id    = t.second;
+	    
+	    // Count all non-normal tokens in the vocab while iterating
             if (!(vocab.id_to_token[id].attr & LLAMA_TOKEN_ATTR_NORMAL)) {
-                vocab.cache_special_tokens.push_back(id);
+		special_tokens_count_by_type++;
+	    }
+
+	    // Skip single character tokens
+	    if (token.length() > 1) {
+		bool is_tokenizable = false;
+
+		// Split token string representation in two, in all possible ways
+		//  and check if both halves can be matched to a valid token
+		for (unsigned i = 1; i < token.length();) {
+		    const auto left  = token.substr(0, i);
+		    const auto right = token.substr(i);
+
+		    // check if we didnt partition in the middle of a utf sequence
+		    auto utf = utf8_len(left.at(left.length() - 1));
+
+		    if (utf == 1) {
+			if (vocab.token_to_id.find(left)  != vocab.token_to_id.end() &&
+			    vocab.token_to_id.find(right) != vocab.token_to_id.end() ) {
+			    is_tokenizable = true;
+			    break;
+			}
+			i++;
+		    } else {
+			// skip over the rest of multibyte utf sequence
+			i += utf - 1;
             }
         }
+		if (!is_tokenizable) {
+		    // Some tokens are multibyte, but they are utf sequences with equivalent text length of 1
+		    //  it's faster to re-filter them here, since there are way less candidates now
 
-        std::sort( vocab.cache_special_tokens.begin(), vocab.cache_special_tokens.end(),
-            [&] (const llama_vocab::id a, const llama_vocab::id b) {
-                return vocab.id_to_token[a].text.size() > vocab.id_to_token[b].text.size();
+		    // Calculate a total "utf" length of a token string representation
+		    size_t utf8_str_len = 0;
+		    for (unsigned i = 0; i < token.length();) {
+			utf8_str_len++;
+			i += utf8_len(token.at(i));
             }
-        );
+		    // And skip the ones which are one character
+		    if (utf8_str_len > 1) {
+			// At this point what we have left are special tokens only
+			vocab.cache_special_tokens[token] = id;
+
+			// Count manually found special tokens
+			special_tokens_count_from_verification++;
+
+			// If this manually found special token is not marked as such, flag a mismatch
+			if (vocab.id_to_token[id].attr == LLAMA_TOKEN_ATTR_NORMAL) {
+			    special_tokens_definition_mismatch = true;
 
-        LLAMA_LOG_INFO("%s: special tokens cache size = %u\n", __func__, (uint32_t)vocab.cache_special_tokens.size());
+	    }
+
+		    }
+		}
+	    }
+	    }
+
+	if (special_tokens_definition_mismatch || special_tokens_count_from_verification != special_tokens_count_by_type) {
+	    LLAMA_LOG_WARN("%s: mismatch in special tokens definition ( %u/%zu vs %u/%zu ).\n",
+		__func__,
+		special_tokens_count_from_verification, vocab.id_to_token.size(),
+		special_tokens_count_by_type, vocab.id_to_token.size()
+	);
+	} else {
+	    LLAMA_LOG_INFO("%s: special tokens definition check successful ( %u/%zu ).\n",
+		__func__,
+		special_tokens_count_from_verification, vocab.id_to_token.size()
+	    );
+	}
     }
 
     // build token to piece cache
@@ -4919,7 +4997,7 @@
             _set_token_attr("<mask>", LLAMA_TOKEN_ATTR_LSTRIP, true);
         } else if (_contains_any(model_name, {"phi-3", "phi3"})) {
             for (auto id : vocab.cache_special_tokens) {
-                _set_tokenid_attr(id, LLAMA_TOKEN_ATTR_RSTRIP, true);
+		_set_tokenid_attr(id.second, LLAMA_TOKEN_ATTR_RSTRIP, true);
             }
             for (auto token : {"</s>"}) {
                 _set_token_attr(token, LLAMA_TOKEN_ATTR_RSTRIP, true);
@@ -13174,7 +13252,7 @@
     llm_tokenizer_wpm(const llama_vocab & vocab): vocab(vocab) {}
 
     void tokenize(const std::string & text, std::vector<llama_vocab::id> & output) {
-        const auto & token_map = vocab.token_to_id;
+	auto * token_map = &vocab.token_to_id;
 
         // normalize and split by whitespace
         std::vector<std::string> words = preprocess(text);
@@ -13189,89 +13267,108 @@
             }
 
             // prepend phantom space
-            const std::string word1 = "\xe2\x96\x81" + word;
-            const int n = word1.size();
-
-            const size_t current_tokens = output.size();
+	    std::string word1 = "\xe2\x96\x81" + word;
+	    int n = word1.size();
 
             // we're at the start of a new word
+	    int i = 0;
+	    bool match_any = false;
+
             // move through character position in word
-            for (int i = 0; i < n; ++i) {
+	    while (i < n) {
                 // loop through possible match length
                 bool match = false;
                 for (int j = n; j > i; j--) {
-                    auto it = token_map.find(word1.substr(i, j - i));
-                    if (it != token_map.end()) {
+		    auto it = token_map->find(word1.substr(i, j - i));
+		    if (it != token_map->end()) {
                         output.push_back(it->second);
                         match = true;
-                        i = j - 1;
+			match_any = true;
+			i = j;
                         break;
                     }
                 }
 
-                if (!match) { // discard all
-                    output.resize(current_tokens);
-                    break;  // and discard next tokens
+		// must be an unknown character
+		if (!match) {
+		    i++;
                 }
             }
 
             // we didn't find any matches for this word
-            if (current_tokens == output.size()) {
+	    if (!match_any) {
                 output.push_back(vocab.special_unk_id);
             }
         }
     }
 
     std::vector<std::string> preprocess(const std::string & text) {
-        const std::vector<uint32_t> cpts_nfd = unicode_cpts_normalize_nfd(unicode_cpts_from_utf8(text));
-        std::vector<std::string> words(1, "");
-
-        for (const char32_t cpt : cpts_nfd) {
-            const auto flags = unicode_cpt_flags(cpt);
+	std::vector<uint32_t> cpts_nfd = unicode_cpts_normalize_nfd(unicode_cpts_from_utf8(text));
 
-            if (flags.is_whitespace) {
-                if (words.back().size()) {  // finish previous word if any
-                    words.emplace_back();
-                }
+	// strip accents, strip control, uniformize whitespace,
+	// to lowercase, pad chinese characters, pad punctuation
+	std::string new_str = "";
+	for (uint32_t code : cpts_nfd) {
+	    const codepoint_flags flags = unicode_cpt_flags(code);
+	    if (flags.is_accent_mark || flags.is_control) {
                 continue;
             }
-
-            assert (!flags.is_separator);
-            if (cpt == 0 || cpt == 0xFFFD || flags.is_control) {
-                continue;
-            }
-
-            const std::string s = unicode_cpt_to_utf8(unicode_tolower(cpt));
-            if (flags.is_punctuation || ( cpt < 0x7F && flags.is_symbol ) || is_chinese_char(cpt)) {
-                if (words.back().size()) {  // finish previous word if any
-                    words.emplace_back();
+	    code = unicode_tolower(code);
+	    if (flags.is_separator || flags.is_whitespace) {  //####FIXME: is_separator ?
+		code = ' ';
                 }
-                words.back() = s;       // single char word
-                words.emplace_back();   // start a new word
+	    std::string s = unicode_cpt_to_utf8(code);
+	    if (flags.is_punctuation || is_ascii_punct(code) || is_chinese_char(code)) {
+		new_str += " ";
+		new_str += s;
+		new_str += " ";
             } else {
-                words.back() += s;  // append char to word
+		new_str += s;
             }
         }
 
-        if (!words.back().size()) {
-            words.pop_back();
+	// split by whitespace
+	uint64_t l = 0;
+	uint64_t r = 0;
+	std::vector<std::string> words;
+	while (r < new_str.size()) {
+	    // if is whitespace
+	    if (isspace(new_str[r], std::locale::classic())) {
+		if (r > l) words.push_back(new_str.substr(l, (r - l)));
+		l = r + 1;
+		r = l;
+	    } else {
+		r += 1;
+	    }
+	}
+	if (r > l) {
+	    words.push_back(new_str.substr(l, (r - l)));
         }
-
         return words;
     }
 
-    static bool is_chinese_char(uint32_t cpt) {
-        return
-            (cpt >= 0x04E00 && cpt <= 0x09FFF) ||
-            (cpt >= 0x03400 && cpt <= 0x04DBF) ||
+    bool is_ascii_punct(uint32_t code) {
+	if (code > 0xFF) {
+	    return false;
+	}
+	auto c = char(static_cast<unsigned char>(code));
+	return ispunct(c, std::locale::classic());
+    }
+
+    bool is_chinese_char(uint32_t cpt) {
+	if ((cpt >= 0x4E00  && cpt <= 0x9FFF)  ||
+	    (cpt >= 0x3400  && cpt <= 0x4DBF)  ||
             (cpt >= 0x20000 && cpt <= 0x2A6DF) ||
             (cpt >= 0x2A700 && cpt <= 0x2B73F) ||
             (cpt >= 0x2B740 && cpt <= 0x2B81F) ||
             (cpt >= 0x2B920 && cpt <= 0x2CEAF) || // this should be 0x2B820 but in hf rust code it is 0x2B920
-            (cpt >= 0x0F900 && cpt <= 0x0FAFF) ||
-            (cpt >= 0x2F800 && cpt <= 0x2FA1F);
-            //(cpt >= 0x3000  && cpt <= 0x303F)  ||
-            //(cpt >= 0xFF00  && cpt <= 0xFFEF);
+	    (cpt >= 0xF900  && cpt <= 0xFAFF)  ||
+	    (cpt >= 0x2F800 && cpt <= 0x2FA1F) ||
+	    (cpt >= 0x3000  && cpt <= 0x303F)  ||
+	    (cpt >= 0xFF00  && cpt <= 0xFFEF)) {
+	    return true; // NOLINT
+	}
+	return false;
     }
 
     const llama_vocab & vocab;
@@ -13315,9 +13412,14 @@
 
 static void tokenizer_st_partition(const llama_vocab & vocab, std::forward_list<fragment_buffer_variant> & buffer) {
     // for each special token
-    for (const llama_vocab::id special_id : vocab.cache_special_tokens) {
+    for (const auto & st: vocab.cache_special_tokens) {
+      const auto & special_token = st.first;
+      const auto & special_id    = st.second;
         const auto & data = vocab.id_to_token[special_id];
-        const auto & special_token = data.text;
+
+    //for (const llama_vocab::id special_id : vocab.cache_special_tokens) {
+    //    const auto & data = vocab.id_to_token[special_id];
+    //    const auto & special_token = data.text;
 
         // for each text fragment
         std::forward_list<fragment_buffer_variant>::iterator it = buffer.begin();
@@ -13326,7 +13428,7 @@
 
             // if a fragment is text ( not yet processed )
             if (fragment.type == FRAGMENT_BUFFER_VARIANT_TYPE_RAW_TEXT) {
-                auto & raw_text = fragment.raw_text;
+		auto * raw_text = &(fragment.raw_text);
 
                 auto raw_text_base_offset = fragment.offset;
                 auto raw_text_base_length = fragment.length;
@@ -13336,7 +13438,7 @@
                     // find the first occurrence of a given special token in this fragment
                     //  passing offset argument only limit the "search area" but match coordinates
                     //  are still relative to the source full raw_text
-                    auto match = raw_text.find(special_token, raw_text_base_offset);
+		    auto match = raw_text->find(special_token, raw_text_base_offset);
 
                     // no occurrences found, stop processing this fragment for a given special token
                     if (match == std::string::npos) break;
@@ -13357,13 +13459,13 @@
                         int64_t left_reminder_length = match - raw_text_base_offset;
 
                         if (data.attr & LLAMA_TOKEN_ATTR_LSTRIP) {
-                            while (left_reminder_length > 0 && isspace(raw_text[left_reminder_offset + left_reminder_length - 1])) {
+			    while (left_reminder_length > 0 && isspace((*raw_text)[left_reminder_offset + left_reminder_length - 1])) {
                                 left_reminder_length--;
                             }
                         }
 
                         if (left_reminder_length > 0) {
-                            buffer.emplace_after(it, raw_text, left_reminder_offset, left_reminder_length);
+			    buffer.emplace_after(it, (*raw_text), left_reminder_offset, left_reminder_length);
                             it++;
                         }
 
@@ -13382,14 +13484,14 @@
                         int64_t right_reminder_length = raw_text_base_length - ((match - raw_text_base_offset) + special_token.length());
 
                         if (data.attr & LLAMA_TOKEN_ATTR_RSTRIP) {
-                            while (right_reminder_length > 0 && isspace(raw_text[right_reminder_offset])) {
+			    while (right_reminder_length > 0 && isspace((*raw_text)[right_reminder_offset])) {
                                 right_reminder_offset++;
                                 right_reminder_length--;
                             }
                         }
 
                         if (right_reminder_length > 0) {
-                            buffer.emplace_after(it, raw_text, right_reminder_offset, right_reminder_length);
+			    buffer.emplace_after(it, (*raw_text), right_reminder_offset, right_reminder_length);
                             it++;
                         }
 
